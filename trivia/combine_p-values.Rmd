---
title: "Global tests combining p-values"
author: "GAO Zheng"
date: "October 31, 2016"
output: html_document
---

```{r, echo=FALSE,results='hide'}
# Kolmogorov-Smirnov Test

KS <- function(x,n) {
  sorted.x = sort(x)
  unf.U = 1:n / n
  unf.L = unf.U - 1 / n
  max(abs(c(sorted.x - unf.L, sorted.x - unf.U)))
}

n = 100
x = runif(n);
sorted.x = sort(x)
unf.U = 1:n / n
unf.L = unf.U - 1 / n
max(abs(c(sorted.x - unf.L, sorted.x - unf.U)))
#plot(1:n / n,sort(x));abline(a = 0, b = 1);
#abline(v = which.max(abs(c(
#  sorted.x - unf.L, sorted.x - unf.U
#))) %% n / n,lty = 2)
```

```{r,fig.height=6}
# Calibrate KS cutoffs
# n = 10000
# KS.cut.off <- c()
# for(n in 10^(1:4)){
#   KS.simulated = replicate(100000,KS(runif(n),n))
#   KS.cut.off[log(n,10)] <- quantile(KS.simulated,probs = 0.95)
# }
KS.cut.off <- c(0.41050279, 0.13409342, 0.04284884, 0.01356758)

fractions <- c(1,0.9,0.8,0.7,0.6)
dimensions <- 10^(1:4)

## storage
typeIIerrorNaive = matrix(0,length(dimensions),length(fractions))
typeIIerrorFisher = matrix(0,length(dimensions),length(fractions))
typeIIerrorEdgington = matrix(0,length(dimensions),length(fractions))
typeIIerrorChisq = matrix(0,length(dimensions),length(fractions)) # bad
typeIIerrorKS = matrix(0,length(dimensions),length(fractions)) # bad

# From low to high dimensions, up to 10000 r.v.'s
for (i in 1:4) {
  for (j in 1:5) {
    n = dimensions[i]
    # play with these two parameters
    shift = -2 / sqrt(n)
    fraction = fractions[j]
    # always simulate 1000 replications
    N = 10 ^ 3
    
    # simulate original data
    d1 = matrix(rnorm(n * N),n,N)
    z1 = pnorm(d1)
    # inject distributed signal
    d2 = d1 + c(rep(shift,n * fraction),rep(0,n - n * fraction))
    z2 = pnorm(d2)
    
    typeIIerrorNaive[i,j] = sum(apply(d2, 2, sum) > -1.65 * sqrt(n))
    typeIIerrorFisher[i,j] = sum(apply(log(z2), 2, sum) > -qchisq(0.95,2 * n) / 2)
    typeIIerrorEdgington[i,j] = sum(apply(z2, 2, sum) > qnorm(0.05,n / 2,sqrt(n / 12)))
    typeIIerrorChisq[i,j] = sum(apply(d2 ^ 2, 2, sum) < qchisq(0.95,n))
    typeIIerrorKS[i,j] = sum(apply(z2, 2, KS, n) < KS.cut.off[log(n,10)])
  }
}

# calculate the type-II error rates
typeIIerrorNaive <- typeIIerrorNaive / N
typeIIerrorFisher <- typeIIerrorFisher / N
typeIIerrorEdgington <- typeIIerrorEdgington / N
typeIIerrorChisq <- typeIIerrorChisq / N
typeIIerrorKS <- typeIIerrorKS / N

# the Higher Criticism statistics. Results from Shen Jinqi, simulating under same settings
typeIIerrorHC <- matrix(c(0.9152, 0.9060, 0.8908, 0.8812, 0.8602,
                          0.9302, 0.9308, 0.9288, 0.9246, 0.9192,
                          0.9552, 0.9560, 0.9604, 0.9498, 0.9548,
                          0.9472, 0.9390, 0.9388, 0.9428, 0.9376),ncol = 5,byrow = T)

j = 2
matplot(
  cbind(
    typeIIerrorNaive[,j],
    typeIIerrorFisher[,j],
    typeIIerrorEdgington[,j],
    typeIIerrorChisq[,j],
    typeIIerrorKS[,j],
    typeIIerrorHC[,j]
  ),
  type = 'b',ylim = c(0,1),
  ylab = "Type II error",
  xaxt = 'n',
  xlab = "Number of hypotheses",
  main = paste("Fractions of Non-nulls ",fractions[j])
)
axis(1,at = 1:4,labels = 10 ^ (1:4))
legend(
  "bottomright",legend = c(
    "NP test","Fisher's test",
    "Edgington's","Chisq test",
    "K-S test","HC test"
  ),
  col = 1:6,pch = as.character(1:6)
)

```