# jemdoc: menu{MENU}{software/index.html}{../}
# jemdoc: addcss{../jemdoc.css}
= Deep Q-Learning 

Here is an agent I trained with [https://github.com/openai/baselines Baselines] using deep Q-learning introduced in [http://www.davidqiu.com:8888/research/nature14236.pdf Human-level control through deep reinforcement learning]. This agent, receiving as input only pixels and game score, learned to play Breakout at human level.

== Results
Here is how the agent performs at various stages of the training process. Each episode corresponds to one ``life'' in the game of breakout. The learning process is rather inefficient in my opinion. The bottom right figure took 2 million episodes and 12 hours on an eight-core 2.60 GHz Intel Xeon E5-2670 processor, and learning seems to be stagnant after a certain amount of episodes, because the agent on the right doesn't seem to out-perform the agent in the middle by any means. Besides, the agents behave very differently from human players, and lack traditional ``intelligence'' expected in a breakout game such as position anticipation, using the middle of the bar for speed control. 
~~~
{}{raw}
<table style="width:90%">
  <tr>
    <td>0.4m episodes</td>
    <td>0.8m episodes</td> 
    <td>2m episodes</td>
  </tr>
  <tr>
    <td><embed src="bo40_2hours.mp4" autostart="true" height="200" width="160" /></td>
    <td><embed src="bo80_5hours.mp4" autostart="false" height="200" width="160" /></td> 
    <td><embed src="pong200.mp4" autostart="false" height="200" width="160" /></td>
  </tr>
</table>
~~~